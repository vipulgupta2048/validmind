{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building Your Own Dataset: Web Scraping with Scrapy in Jupyter Notebooks\n",
        "\n",
        "Training machine learning models require accurate datasets from the right sources. One way to procure, process and validate the data needed for a dataset is to do it yourself through web scraping.\n",
        "\n",
        "Following this guide, we will be building a Scrapy spider to scrape data from a website, learn fundamentals of scrapy, and load the scraped JSON data into a Pandas dataframe all in a Jupyter notebook to start creating our own dataset.\n"
      ],
      "metadata": {
        "id": "79nqhJ6PxInu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "### Tools used\n",
        "\n",
        "Ensure that you have the following tools installed in your system:\n",
        "\n",
        "1. [Python 3](https://realpython.com/installing-python/)\n",
        "3. [Jupyter Notebook](https://jupyter.org/install)\n",
        "\n",
        "Once complete, open the Jupyter notebook, and validate if your installation was successful by running the following commands:\n",
        "\n"
      ],
      "metadata": {
        "id": "iQKtW77A7q95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Python version\n",
        "import platform\n",
        "import jupyter_core\n",
        "\n",
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Jupyter Version: {jupyter_core.__version__}\")\n"
      ],
      "metadata": {
        "id": "U6lTkZG4pDU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing dependencies\n",
        "\n",
        "Install the following Python packages using `pip3`, the Python package installer for Python\n",
        "\n",
        "> Install the dependencies to a [virtual environment](https://docs.python.org/3/library/venv.html) is highly recommended to not interfere with system level packages and cause issues. [Guide to create virtual environments](https://realpython.com/python-virtual-environments-a-primer/).\n",
        "\n",
        "1. [Scrapy](https://docs.scrapy.org/en/latest): Web crawling framework used for web scraping.\n",
        "2. [Crochet](https://pypi.org/project/crochet/): Helper package to unblock Twisted in Jupyter Notebook environments"
      ],
      "metadata": {
        "id": "xFjABI5F9tXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yw2xliYoAgt"
      },
      "outputs": [],
      "source": [
        "# Install scrapy, crochet\n",
        "!pip3 install scrapy==2.11.2\n",
        "!pip3 install crochet==2.1.1\n",
        "\n",
        "# import Scrapy\n",
        "import scrapy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To unblock Twisted code running in Jupyter notebook environment, we use Crochet to setup our project and not face the `ReactorNotRestartable` error."
      ],
      "metadata": {
        "id": "g0J42P28BCxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reactor restart\n",
        "from crochet import setup, wait_for\n",
        "setup()"
      ],
      "metadata": {
        "id": "_i1fr9mhBFjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our Spider\n",
        "\n",
        "In this guide, we are using the `ValidMindSpider` to extract blog post metadata and content from [https://validmind.com/blog/](https://validmind.com/blog/). We will be going step-by-step gathering blog post links, extracting content using [XPath selectors](https://docs.scrapy.org/en/2.11/topics/selectors.html), and paginate recursively through pages to repeat the same process.\n",
        "\n",
        "### Finding all blog post links on the current page\n",
        "\n",
        "Scrapy accepts `start_urls` list as input to start scraping content from a website URL. The `parse()` method is the default callback that runs for each URL in the list. In the method, we define HTML elements to be picked up. In this case our XPath selector, `//div[contains(@class, \"x-row-inner\")][1]/a/@href` picks up all links from the `div` container having `x-row-inner` class. Learn more about [XPath Selectors](https://bugbug.io/blog/testing-frameworks/the-ultimate-xpath-cheat-sheet/).\n",
        "\n",
        "Run the following code to extract the links\n"
      ],
      "metadata": {
        "id": "B31L2VukBk2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapy.spiders import Spider\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "\n",
        "class ValidMindSpider(Spider):\n",
        "    name = 'valid'\n",
        "    start_urls = ['https://validmind.com/blog/']  # FIRST LEVEL\n",
        "\n",
        "    custom_settings = {\n",
        "        \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n",
        "    }\n",
        "\n",
        "    # 1. FOLLOWING LINKS\n",
        "    def parse(self, response):\n",
        "        # Extract all the blog post links from the current page\n",
        "        links = response.xpath('//div[contains(@class, \"x-row-inner\")][1]/a/@href').getall()\n",
        "\n",
        "        # Extract data from each blog post on the blog page\n",
        "        for link in links:\n",
        "            print(link)"
      ],
      "metadata": {
        "id": "XsyniMN6oKnh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the spider, we create a new instance of the CrawlerRunner. CrawlerRunner is a thin wrapper that encapsulates helpers to run multiple crawlers without interferring with existing reactors in any way. We will be using the `run_spider()` function to run our `ValidMindSpider`."
      ],
      "metadata": {
        "id": "PVw__6ULEgjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapy.crawler import CrawlerRunner\n",
        "\n",
        "@wait_for(10)\n",
        "def run_spider(spider):\n",
        "    crawler = CrawlerRunner()\n",
        "    return crawler.crawl(spider)\n",
        "\n",
        "run_spider(ValidMindSpider)"
      ],
      "metadata": {
        "id": "wwpZ0KT6Ecmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting content from blogs\n",
        "\n",
        "After extracting blog links from the page, we need to dive deeper and extract the content of each blog post.\n",
        "\n",
        "To do that, we create the `extract_blog` method. For each post, we are looking to extract the heading, date, author and content. Each of these elements on the HTML response can be picked up using XPath selectors."
      ],
      "metadata": {
        "id": "_2MgF5KbGZLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapy.spiders import Spider\n",
        "\n",
        "class ValidMindSpider(Spider):\n",
        "    name = 'valid'\n",
        "    start_urls = ['https://validmind.com/blog/']  # FIRST LEVEL\n",
        "\n",
        "    custom_settings = {\n",
        "        \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n",
        "    }\n",
        "\n",
        "    # 1. FOLLOWING\n",
        "    def parse(self, response):\n",
        "        # Extract all the blog post links from the current page\n",
        "        links = response.xpath('//div[contains(@class, \"x-row-inner\")][1]/a/@href').getall()\n",
        "\n",
        "        # Extract data from each blog post on the blog page\n",
        "        for link in links:\n",
        "            yield response.follow(link, self.extract_blog)\n",
        "\n",
        "    # 2. DATA EXTRACTION\n",
        "    def extract_blog(self, response):\n",
        "        # Extract blog post data from the individual blog post\n",
        "        data = {\n",
        "            \"heading\": response.xpath('//h1/text()').get(),\n",
        "            \"author\": response.xpath('//div[contains(@class, \"pp-author-boxes-name\")]/a/@title').get(),\n",
        "            \"date\": response.xpath('//div[@class=\"x-text-content-text\"]//span[@class=\"x-text-content-text-subheadline\"]/text()').get(),\n",
        "            # \"content\": response.xpath('//div[@class=\"x-text x-content e129323-e11 m2rsb-y m2rsb-z\"]/p/text()').getall()\n",
        "        }\n",
        "        print(data)\n",
        "\n",
        "run_spider(ValidMindSpider)"
      ],
      "metadata": {
        "id": "Zb66guzDChOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paginate through pages: Get all the data\n",
        "\n",
        "[Pagination in Scrapy](https://scrapeops.io/python-scrapy-playbook/scrapy-pagination-guide/) is a critical skill. It allows scrapers to traverse websites like a user would and access data needed to be scraped. We opted for static URLs to paginate through in this website to keep it straightforward.\n",
        "\n",
        "> Ideally, pagination in websites can be upto hundreds or thousands of pages. Hence, making sure you [dynamically select pagination URLs](https://docs.scrapy.org/en/2.11/topics/dynamic-content.html) is imperative for the reliablity of your spider.\n",
        "\n",
        "The `pagination_links` list URLs are pages to paginate through. Once all links on the current page are finished extracting, we pick the next page from the list in `next_page_url` which is then used to recursively scrape all the blogs on the next page.\n",
        "\n",
        "The `custom_settings` allow specific control over scrapy data output. We used `FEEDS` to create a file to store JSON data."
      ],
      "metadata": {
        "id": "mCxKs8RjHwOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapy.spiders import Spider\n",
        "\n",
        "class ValidMindSpider(Spider):\n",
        "    name = 'valid'\n",
        "    start_urls = ['https://validmind.com/blog/']  # FIRST LEVEL\n",
        "    pagination_links = ['https://validmind.com/blog/?_paged=2', 'https://validmind.com/blog/?_paged=3', 'https://validmind.com/blog/?_paged=4', 'https://validmind.com/blog/?_paged=5']\n",
        "\n",
        "    custom_settings = {\n",
        "        \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n",
        "        'FEEDS': {\n",
        "            'validmind-blogs.json': {\n",
        "                'format': 'json',\n",
        "                'overwrite': True\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # 1. FOLLOWING\n",
        "    def parse(self, response):\n",
        "        # Extract all the blog post links from the current page\n",
        "        links = response.xpath('//div[contains(@class, \"x-row-inner\")][1]/a/@href').getall()\n",
        "\n",
        "        # Extract data from each blog post on the blog page\n",
        "        for link in links:\n",
        "            yield response.follow(link, self.extract_blog)\n",
        "\n",
        "        # 3. PAGINATION\n",
        "        # Paginate through the next page if available\n",
        "        next_page_url = self.pagination_links.pop(0) if self.pagination_links else None\n",
        "\n",
        "        # Recursively follow the next page if available\n",
        "        if next_page_url:\n",
        "            yield response.follow(next_page_url, self.parse)\n",
        "\n",
        "    # 2. DATA EXTRACTION\n",
        "    def extract_blog(self, response):\n",
        "        # Extract blog post data from the individual blog post\n",
        "        data = {\n",
        "            \"heading\": response.xpath('//h1/text()').get(),\n",
        "            \"author\": response.xpath('//div[contains(@class, \"pp-author-boxes-name\")]/a/@title').get(),\n",
        "            \"date\": response.xpath('//div[@class=\"x-text-content-text\"]//span[@class=\"x-text-content-text-subheadline\"]/text()').get(),\n",
        "            # \"content\": response.xpath('//div[@class=\"x-text x-content e129323-e11 m2rsb-y m2rsb-z\"]/p/text()').getall()\n",
        "        }\n",
        "        return data\n",
        "\n",
        "\n",
        "run_spider(ValidMindSpider)"
      ],
      "metadata": {
        "id": "q1oaMHfBorXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Once the `ValidMindSpider` finishes its run, a JSON file will be created. Load the JSON data using pandas to further analyze, modify and process the data you have scraped to start training your model on the dataset or get valuable insights from it.\n"
      ],
      "metadata": {
        "id": "kTvXEzlQJ9UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "validjson = pd.read_json('validmind-blogs.json')\n",
        "validjson"
      ],
      "metadata": {
        "id": "OF9Ax3pap0jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this guide, we went over the basics of building your own datasets using Scrapy and Jupyter Notebooks. We scraped through a website, using XPath selectors and pagination. We stored the scraped data in JSON, and presented the data in a Jupyter Notebook."
      ],
      "metadata": {
        "id": "Q_y7RgamLQiC"
      }
    }
  ]
}